{
  "hash": "83056525da6222062c5432777bc69370",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Regression modelling\"\nsubtitle: ENVX2001 Applied Statistical Methods\nauthor: Liana Pozza\ninstitute: The University of Sydney\ndate: last-modified # today | last-modified\ndate-format: \"MMM YYYY\"\nexecute:\n  cache: false\n  echo: true\neditor-options:\n  canonical: true\ntoc: true\ntoc-depth: 1\ntoc-title: Outline\n---\n\n\n\n\n\n# Brief history\n\n\n\n![Adrien-Marie Legendre](assets/legendre.jpg)\n![Carl Friedrich Gauss](assets/gauss.jpg)\n![Francis Galton](assets/galton.jpg)\n\nAdrien-Marie Legendre, Carl Friedrich Gauss & Francis Galton\n\n\n\n\n## Least squares, correlation and astronomy\n\n- Method of least squares first [**published**]{style=\"color: firebrick\"} paper by Adrien-Marie Legendre in 1805\n- Technique of least squares used by Carl Friedrich Gauss in 1809 to fit a parabola to the orbit of the asteroid Ceres\n- Model fitting first [**published**]{style=\"color: firebrick\"} by Francis Galton in 1886 to the problem of predicting the height of a child from the height of the parents\n\n:::{.callout-note}\nMany other people contributed to the development of regression analysis, but these three are the \"most\" well-known.\n:::\n\n## Galton's data \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(HistData)\ndplyr::tibble(Galton)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 928 × 2\n   parent child\n    <dbl> <dbl>\n 1   70.5  61.7\n 2   68.5  61.7\n 3   65.5  61.7\n 4   64.5  61.7\n 5   64    61.7\n 6   67.5  62.2\n 7   67.5  62.2\n 8   67.5  62.2\n 9   66.5  62.2\n10   66.5  62.2\n# ℹ 918 more rows\n```\n\n\n:::\n:::\n\n\n- 928 children of 205 pairs of parents\n- Height of parents and children measured in inches\n- Size classes were binned (hence data looks discrete)\n\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(Galton, aes(x = parent, y = child)) +\n  geom_point(alpha = .2, size = 3)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(Galton, aes(x = parent, y = child)) +\n  geom_point(alpha = .2, size = 3) + geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n## Regression modelling in R\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(child ~ parent, data = Galton)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.94153    2.81088   8.517   <2e-16 ***\nparent       0.64629    0.04114  15.711   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,\tAdjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n<br> \n\nThat's it... you have fitted a model! \n\n. . .\n\n:::{.callout-caution}\n### Question\nBut how do we assess the quality of the model?\n:::\n\n\n<!-- ```{r}\nlibrary(report)\nreport(fit)\n```  -->\n\n# Simple linear regression\n\n## Defining a linear relationship {.nostretch}\n\n-  Pearson correlation coefficient measures the linear correlation between two variables\n- Does not distinguish different *patterns* of association, only the *strength* of the association\n\n![](assets/correlation.png)\n\n- Not quite usable for *predictive* modelling, or for *inference* about the relationship between variables\n\n\n## Anscombe's quartet\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nanscombe %>%\n  pivot_longer(everything(), cols_vary = \"slowest\",\n    names_to = c(\".value\", \"set\"), names_pattern = \"(.)(.)\") %>%\n  ggplot(aes(x = x, y = y)) +\n    geom_point(size = 3) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    facet_wrap(~set, ncol = 4)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n*All of these data have a correlation coefficient of about 0.8, but **only one** of them meet the assumptions of a linear model.*\n\n## Datasaurus Dozen\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(datasauRus)\nggplot(datasaurus_dozen, aes(x=x, y=y)) +\n  geom_point(size = .5, alpha = .3) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  facet_wrap(~dataset, ncol = 6)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n*All of these data have the **same** correlation coefficient, mean and s.d. but look vastly different.*\n\n## Simple linear regression modelling {auto-animate=\"true\"}\n\nWe want to predict an outcome $Y$ based on a predictor $x$ for $i$ number of observations: \n\n$$Y_i = \\color{royalblue}{\\beta_0 + \\beta_1 x_i} +\\color{red}{\\epsilon_i}$$\n\nwhere\n\n$$\\epsilon_i \\sim N(0, \\sigma^2)$$\n\n- $Y_i$, the *response*, is an observed value of the dependent variable.\n- $\\beta_0$, the *constant*, is the population intercept and is **fixed**.\n- $\\beta_1$ is the population *slope* parameter, and like $\\beta_0$, is also **fixed**.\n- $\\epsilon_i$ is the error associated with predictions of $y_i$, and unlike $\\beta_0$ or $\\beta_1$, it is *not fixed*.\n\n. . .\n\n:::{.callout-note}\n[We tend to associate $\\epsilon_i$ with the **residual**, which is a positive or negative difference from the \"predicted\" response, rather than error itself which is a difference from the **true** response]{style=\"color: red;\"}\n:::\n\n## Interpreting the relationship {auto-animate=\"true\"}\n\n$$Y_i = \\color{royalblue}{\\beta_0 + \\beta_1 x_i} +\\color{red}{\\epsilon_i}$$\n\n[Basically, a *deterministic* straight line equation $y = c + mx$, with added *random* variation that is normally distributed]{style=\"color: seagreen;\"}\n\n. . .\n\n- Response = [Prediction]{style=\"color: royalblue\"} + [Error]{style=\"color: red\"}\n- Response = [Signal]{style=\"color: royalblue\"} + [Noise]{style=\"color: red\"}\n- Response = [Model]{style=\"color: royalblue\"} + [Unexplained]{style=\"color: red\"}\n- Response = [Deterministic]{style=\"color: royalblue\"} + [Random]{style=\"color: red\"}\n- Response = [Explainable]{style=\"color: royalblue\"} + [Everything else]{style=\"color: red\"}\n\n\n\n\n## Fitting the model {auto-animate=\"true\"}\n\n- The *residual* is the difference between the observed value of the response and the predicted value:\n\n$$\\hat\\epsilon_i = y_i - \\color{royalblue}{\\hat{y}_i}$$\n\n. . .\n\nwhere $\\color{royalblue}{\\hat{y}_i}$ is the predicted value of $y_i$:\n\n$$\\color{royalblue}{\\hat{y}_i} = \\beta_0 + \\beta_1 x_i$$\n\n. . .\n\ntherefore:\n\n$$\\hat\\epsilon_i = y_i - \\color{royalblue}{(\\beta_0 + \\beta_1 x_i)}$$\n\n. . .\n\n- We use the **method of least squares** and minimise the sum of the squared residuals (SSR):\n\n$$\\sum_{i=1}^n \\hat\\epsilon_i^2 = \\sum_{i=1}^n (y_i - \\color{royalblue}{(\\beta_0 + \\beta_1 x_i)})^2$$\n\n## {auto-animate=\"true\"}\n\n$$\\sum_{i=1}^n \\hat\\epsilon_i^2 = \\sum_{i=1}^n (y_i - \\color{royalblue}{(\\beta_0 + \\beta_1 x_i)})^2$$\n\n. . .\n\nFinding the minimum SSR requires solving the following problem:\n\n$$\\color{firebrick}{argmin_{\\beta_0, \\beta_1}} \\sum_{i=1}^n (y_i - \\color{royalblue}{(\\beta_0 + \\beta_1 x_i)})^2$$\n\n## {auto-animate=\"true\"}\n\n$$\\color{firebrick}{argmin_{\\beta_0, \\beta_1}} \\sum_{i=1}^n (y_i - \\color{royalblue}{(\\beta_0 + \\beta_1 x_i)})^2$$\n\n\n![[source](https://github.com/Enchufa2/ls-springs)](assets/leastsquares.gif){fig-align=\"center\"}\n\n\n\n## Using `lm()`\n\nLinear regression in R is performed using the `lm()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(child ~ parent, data = Galton)\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nCoefficients:\n(Intercept)       parent  \n    23.9415       0.6463  \n```\n\n\n:::\n:::\n\n\n\n. . .\n\n<br>\n\nDoes the input of `lm()` look familiar? It should!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- aov(y ~ x, data)\n```\n:::\n\n## Handling `lm()` output\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n```\n\n\n:::\n:::\n\n\n. . .\n\nWe can extract the output objects using the `$` operator:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)      parent \n 23.9415302   0.6462906 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit$call\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlm(formula = child ~ parent, data = Galton)\n```\n\n\n:::\n:::\n\n\n## Using external packages to handle `lm()` output\n\n. . .\n\nThe `broom` package simplifies handling of model objects by converting them into tidy data frames:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nsumm_fit <- tidy(fit)\nsumm_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   23.9      2.81        8.52 6.54e-17\n2 parent         0.646    0.0411     15.7  1.73e-49\n```\n\n\n:::\n:::\n\n\n. . .\n\nThe `sjPlot` package is useful to create a summary table:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sjPlot)\nsjPlot::tab_model(fit, dv.labels = \"\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;nodv  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;nodv  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;nodv  \">CI</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;nodv  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">23.94</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">18.43&nbsp;&ndash;&nbsp;29.46</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">parent</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.65</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.57&nbsp;&ndash;&nbsp;0.73</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"3\">928</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> / R<sup>2</sup> adjusted</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"3\">0.210 / 0.210</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n---\n\nThe `glance` function is useful for quickly assessing model parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::glance(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.210         0.210  2.24      247. 1.73e-49     1 -2064. 4133. 4148.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n. . .\n\nThe `augment` function adds the fitted values and residuals to the data frame:\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 928 × 8\n   child parent .fitted .resid    .hat .sigma .cooksd .std.resid\n   <dbl>  <dbl>   <dbl>  <dbl>   <dbl>  <dbl>   <dbl>      <dbl>\n 1  61.7   70.5    69.5  -7.81 0.00270   2.22 0.0165       -3.49\n 2  61.7   68.5    68.2  -6.51 0.00109   2.23 0.00462      -2.91\n 3  61.7   65.5    66.3  -4.57 0.00374   2.23 0.00787      -2.05\n 4  61.7   64.5    65.6  -3.93 0.00597   2.24 0.00931      -1.76\n 5  61.7   64      65.3  -3.60 0.00735   2.24 0.00966      -1.62\n 6  62.2   67.5    67.6  -5.37 0.00130   2.23 0.00374      -2.40\n 7  62.2   67.5    67.6  -5.37 0.00130   2.23 0.00374      -2.40\n 8  62.2   67.5    67.6  -5.37 0.00130   2.23 0.00374      -2.40\n 9  62.2   66.5    66.9  -4.72 0.00218   2.23 0.00487      -2.11\n10  62.2   66.5    66.9  -4.72 0.00218   2.23 0.00487      -2.11\n# ℹ 918 more rows\n```\n\n\n:::\n:::\n\n\n# Assessing model fit\n\n## Assumptions\n\nThe data **must** meet certain criteria, which we often call *assumptions*. They can be remembered using **LINE**:\n\n- **L**inearity. The relationship between $y$ and $x$ is linear.\n- **I**ndependence. The errors $\\epsilon$ are independent.\n- **N**ormal. The errors $\\epsilon$ are normally distributed.\n- **E**qual Variance. At each value of $x$, the variance of $y$ is the same i.e. homoskedasticity, or constant variance.\n\n. . .\n\n**Notice any similarities to the assumptions of ANOVA?**\n\n:::{.callout-tip}\nAll but the independence assumption can be assessed using diagnostic plots. \n:::\n\n## Assumptions with `plot()`\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow= c(2, 2))\nplot(fit)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n## Assumptions using `performance`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(performance)\n```\n:::\n\n\n> With great power....\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::check_model(fit) # check all assumptions\n\n# check specific assumption(s)\nperformance::check_model(fit, check = \"xxx\") \n```\n:::\n\n\n:::{.callout-tip}\nIt might be easier to specify the assumptions using the `check` argument, as the default method might use diagnostic plots that are not always easy to interpret.\n:::\n\n## Assumption: Linearity\n\nPrior knowledge and visual inspection comes into play. Does the relationship look approximately linear?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(Galton, aes(x = parent, y = child)) +\n  geom_point(alpha = .2, size = 3) +\n  # labs(\n  #   x = expression(\"Temperature \" ( degree~C)), \n  #   y = \"Ozone (parts per billion)\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n:::{.callout-tip}\nAfter running the regression, the linearity assumption can be checked *again* by looking at a plot of the residuals against $x$ i.e. size.\n:::\n\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::check_model(fit, check = \"linearity\")\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n- Where the reference line is above 0, the model *underestimates* size, and where it is below 0, it *overestimates* size.\n- If the linearity assumption is **violated**, there is no reason to validate the model since it is no longer suitable for the data.\n\n\n## Assumption: Independence\n\nThis assumption is addressed during experimental design, but issues like correlation between errors and patterns occurring due to time are possible\n\n- Randomisation and proper sampling handles *most* issues with independence\n- Violations may occur if observations of the same subject are related i.e. [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)\n- Violations may occur in time-series data, if the same subjects are sampled i.e. [autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation)\n\n## Assumption: Normality\n\n![](assets/residual.jpg)\n\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::check_model(fit, check = c(\"normality\", \"qq\"))\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n---\n\n## Assessing normality using residuals\n\n- **Light-tailed**: small variance in residuals, resulting in a narrow distribution\n- **Heavy-tailed**: many extreme positive and negative residuals, resulting in a wide distribution\n- **Left-skewed** (n shape): more data falls to the left of the mean\n- **Right-skewed** (u shape): more data falls to the right of the mean\n\n---\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(915)\nx <- rnorm(100)\ny <- 2 + 5 * x + rchisq(100, df = 2)\ndf <- data.frame(x, y)\nperformance::check_model(lm(y ~ x, data = df),\n  check = c(c(\"normality\", \"qq\")))\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n. . .\n\n[Heavy-tailed]{.absolute top=200 left=320}\n[Right-skewed]{.absolute top=340 left=200}\n\n[Heavy-tailed]{.absolute bottom=200 right=0}\n[Right-skewed]{.absolute bottom=300 right=200}\n\n---\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(1028)\nx <- rnorm(100)\ny <- 2 + 5 * x + rchisq(100, df = 3) * -1\ndf <- data.frame(x, y)\nperformance::check_model(lm(y ~ x, data = df),\n  check = c(c(\"normality\", \"qq\")))\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n. . .\n\n[Heavy-tailed]{.absolute bottom=170 left=120}\n[Left-skewed]{.absolute bottom=350 left=250}\n\n[Heavy-tailed]{.absolute bottom=200 right=380}\n[Left-skewed]{.absolute bottom=430 right=200}\n\n---\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(1028)\nx <- rnorm(100)\ny <- 2 + 5 * x + rnbinom(100, 10, .5)\ndf <- data.frame(x, y)\nperformance::check_model(lm(y ~ x, data = df),\n  check = c(c(\"normality\", \"qq\")))\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-24-1.png){width=960}\n:::\n:::\n\n\n. . .\n\n[Light-tailed?]{.absolute bottom=200 left=50}\n[Right-skewed?]{.absolute top=320 left=180}\n[Outlier?]{.absolute top=120 left=400}\n\n[Light-tailed]{.absolute bottom=200 right=320}\n[Outlier?]{.absolute bottom=150 right=0}\n\n## External resources on QQ plots\n\n- [How to interpret a QQ plot](https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot)\n- [QQ plot interpretation](https://math.illinois.edu/system/files/inline-files/Proj9AY1516-report2.pdf)\n\n## Asumption: Equal variances\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::check_model(fit, check = c(\"homogeneity\", \"outliers\"))\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n## What is a standardised residual?\n\n- The standardised residual is the residual divided by the standard error of the residual.\n\n$$Standardised\\ residual = \\frac{Residual}{Standard\\ error\\ of\\ the\\ residual}$$\n\n- Number of standard deviations that the residual is from the *mean* of the residuals.\n- Makes it easy to assess the **equal variances** assumption (among other things).\n  \n# Inference\nWhat can we understand about the relationship between `child` and `parent`?\n\n## Hypothesis testing\n\nHow does our null ($H_0: \\beta_1=0$) model compare to the linear ($H_0: \\beta_1 \\neq 0$) model?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nnull_model <- Galton %>%\n  lm(child ~ 1, data = .) %>%\n  augment(Galton)\nlin_model <- Galton %>%\n  lm(child ~ parent, data = .) %>%\n  augment(Galton)\nmodels <- bind_rows(null_model, lin_model) %>%\n  mutate(model = rep(c(\"Null model\", \"SLR model\"), each = nrow(Galton)))\n\nggplot(data = models, aes(x = parent, y = child)) +\n  geom_smooth(\n    data = filter(models, model == \"Null model\"),\n    method = \"lm\", se = FALSE, formula = y ~ 1, size = 0.5\n  ) +\n  geom_smooth(\n    data = filter(models, model == \"SLR model\"),\n    method = \"lm\", se = FALSE, formula = y ~ x, size = 0.5\n  ) +\n  geom_segment(\n    aes(xend = parent, yend = .fitted),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    size = 0.3, color = \"darkgray\"\n  ) +\n  geom_point(alpha = .2) +\n  facet_wrap(~model) +\n  xlab(\"Parent height (in)\") +\n  ylab(\"Child height (in)\")\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n\n## ANOVA using linear regression \n\nANOVA is simply a variation of the linear regression\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n### Using ANOVA\n\n### `anova(fit)`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(formula = child ~ parent, data = Galton)\nanova(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: child\n           Df Sum Sq Mean Sq F value    Pr(>F)    \nparent      1 1236.9 1236.93  246.84 < 2.2e-16 ***\nResiduals 926 4640.3    5.01                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n### Using Regression\n\n### `summary(fit)`\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.94153    2.81088   8.517   <2e-16 ***\nparent       0.64629    0.04114  15.711   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,\tAdjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n:::\n::::\n\n## ANOVA using linear regression\n\nANOVA is simply a variation of the linear regression\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n### Using ANOVA\n\nThe ANOVA suggests that the main effect of parent is statistically significant and large (F(1, 926) = 246.84, p < .001)\n:::\n\n::: {.column width=\"50%\"}\n### Using Regression\nWe fitted a linear model (estimated using OLS) to predict child with parent (formula: child ~ parent). The model explains a statistically significant and moderate proportion of variance (R2 = 0.21, F(1, 926) = 246.84, p < .001, adj. R2 = 0.21). Within this model, the effect of parent is statistically significant and positive (beta = 0.65, 95% CI [0.57, 0.73], t(926) = 15.71, p < .001).\n\n:::\n::::\n\n# Patterns\nWhat if we want to predict from the data?\n\n## Model fit {auto-animate=\"true\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nCoefficients:\n(Intercept)       parent  \n    23.9415       0.6463  \n```\n\n\n:::\n:::\n\n\nTranslates to:\n\n$$\\widehat{child} = 23.9 + 0.65 \\cdot parent$$\n\n. . .\n\n- For every unit change in parent (i.e. *1 inch*), we expect a 0.65 unit change in child.\n- Note that the model is deterministic, so we can predict the value of child for *any* value of parent, *even if it doesn't make sense -- need to be careful!*\n- Error is no longer \"counted\" in the fit, although it is used to estimate the parameters.\n\n## {auto-animate=\"true\"}\n\n\n$$\\widehat{child} = 23.9 + 0.65 \\cdot parent$$\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.94153    2.81088   8.517   <2e-16 ***\nparent       0.64629    0.04114  15.711   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,\tAdjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n. . .\n\n- **Multiple R^2^**: proportion of variance in the response variable that is explained by the model.\n- **Adjusted R^2^**: R^2^ adjusted for the number of predictors in the model. It only increases if the new term improves the model more than would be expected by chance - a **multiple regression** situation\n    - *always lower than R^2^*\n    - pick this one if you want to compare models\n\n<!-- It is a measure of how far the data points are from the fitted line. -->\n\n## Making predictions\n\nWhat is the predicted child height for a parent height of 70 inches?\n\n. . .\n\n\nSince:\n\n$$\\widehat{child} = 23.9 + 0.65 \\cdot parent$$\n\nThen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchild <- 23.9 + 0.65 * 70\nchild\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 69.4\n```\n\n\n:::\n:::\n\n\n. . .\n\nUse `predict()` to make predictions:\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit, data.frame(parent = 70))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1 \n69.18187 \n```\n\n\n:::\n:::\n\n\n. . .\n\n- Need to consider:\n    - Prediction quality\n    - Prediction performance\n    - **Week 9 - Predictive modelling**\n\n# Transformations\n\nWhat if assumptions are not met, or we want to improve the model?\n\n## What if assumptions are not met?\n\n### Violations of...\n\n- **Linearity** can cause systematically wrong predictions\n- **Homoskedasticity** makes it difficult to estimate \"true\" standard deviation of errors (i.e. noisy estimates)\n- **Normality** can compromise inferences and hypothesis testing\n\n\n## How do we solve these problems?\n\n- Use less restrictive (but more complicated) methods, e.g. generalised linear models, non-parametric techniques \n- Perform variance corrections\n- [**Transform the response variable ($Y$)** to stabilise variance and correct normality]{style=\"color: seagreen\"}\n- [**Transform the predictor variable ($x$)** if issues still exist in the diagnostics]{style=\"color: seagreen\"}\n\n:::{.callout-note}\nWe can also perform transformations to improve the model fit, but **beware of overfitting** -- we want to make reasonable predictions, not fit the data!\n:::\n\n## New example: Air quality\n\nDaily air quality measurements in New York, May to September 1973\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nglimpse(airquality)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 153\nColumns: 6\n$ Ozone   <int> 41, 36, 12, 18, NA, 28, 23, 19, 8, NA, 7, 16, 11, 14, 18, 14, …\n$ Solar.R <int> 190, 118, 149, 313, NA, NA, 299, 99, 19, 194, NA, 256, 290, 27…\n$ Wind    <dbl> 7.4, 8.0, 12.6, 11.5, 14.3, 14.9, 8.6, 13.8, 20.1, 8.6, 6.9, 9…\n$ Temp    <int> 67, 72, 74, 62, 56, 66, 65, 59, 61, 69, 74, 69, 66, 68, 58, 64…\n$ Month   <int> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Day     <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n```\n\n\n:::\n:::\n\n\n## Is Ozone concentration influenced by Temperature?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(airquality, aes(x = Temp, y = Ozone)) +\n  geom_point(alpha = .2, size = 3) +\n  labs(\n    x = expression(\"Temperature \" ( degree~C)), \n    y = \"Ozone (parts per billion)\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-34-1.png){width=960}\n:::\n:::\n\n\n## Assumption checks\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(Ozone ~ Temp, data = airquality)\nlibrary(ggfortify)\nautoplot(fit)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-35-1.png){width=960}\n:::\n:::\n\n\nIs a simple liner model appropriate?\n\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::check_model(fit)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-36-1.png){width=960}\n:::\n:::\n\n\nIs a simple liner model appropriate? \n\n. . .\n\n*Depends on your threshold for what is acceptable.*\n\n## The Log transform\n\n- Log-linear: $Log(Y)=\\beta_0+\\beta_1x$\n  - Good: an increase of $x$ by 1 unit corresponds to a $\\beta_1$ unit increase in $log(Y)$\n  - Simple: an increase of $x$ by 1 unit corresponds to a $\\beta_1 \\times 100\\%$ increase in $Y$\n- Linear-log: $Y=\\beta_0+\\beta_1log(x)$\n  - An increase of $1\\%$ in $x$ corresponds to a $\\frac{\\beta_1}{100}$ increase in $Y$\n- Log-log: $Log(Y)=\\beta_0+\\beta_1log(x)$\n  - An increase of $1\\%$ in $x$ corresponds to a $\\beta_1\\%$ increase in $Y$\n\n## Transforming Ozone\n\nLet's log transform Ozone using the natural log.\n\n:::: {.columns}\n \n::: {.column width=\"50%\"}\n:::{.fragment}\n\n### Before\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(airquality, aes(x = Temp, y = Ozone)) +\n  geom_point(alpha = .2, size = 3) +\n  labs(\n    x = expression(\"Temperature \" ( degree~C)), \n    y = \"Ozone (parts per billion)\") +\n  geom_smooth(method = \"lm\", se = FALSE) \n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-37-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n::: {.column width=\"50%\"}\n:::{.fragment}\n\n### After\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"2\"}\nggplot(airquality, aes(x = Temp, y = log(Ozone))) +\n  geom_point(alpha = .2, size = 3) +\n  labs(\n    x = expression(\"Temperature \" ( degree~C)), \n    y = \"Ozone (parts per billion)\") +\n  geom_smooth(method = \"lm\", se = FALSE) \n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-38-1.png){width=960}\n:::\n:::\n\n:::\n:::\n::::\n\n## Transformations\n\nThe transformed model is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate the transformed variable\nfit_log <- lm(log(Ozone) ~ Temp, data = airquality)\nfit_log\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(Ozone) ~ Temp, data = airquality)\n\nCoefficients:\n(Intercept)         Temp  \n    -1.8380       0.0675  \n```\n\n\n:::\n:::\n\n\n...and the model equation is: \n\n$$\\widehat{log(Ozone)}=\\color{royalblue}{-1.8380 + 0.0675 \\times Temp}$$\n\n. . .\n\n> A 1 degree (&deg;F) increase in temperature is associated with a 6.75% increase in ozone concentration.\n\n## Assumption: Linearity\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n### Before\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit, 1, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-40-1.png){width=960}\n:::\n:::\n\n:::\n\n:::{.column width=\"50%\"}\n\n### After\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit_log, 1, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-41-1.png){width=960}\n:::\n:::\n\n:::\n::::\n\n\n## Assumption: Normality\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n### Before\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit, 2, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-42-1.png){width=960}\n:::\n:::\n\n:::\n\n:::{.column width=\"50%\"}\n\n### After\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit_log, 2, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-43-1.png){width=960}\n:::\n:::\n\n:::\n::::\n\n\n## Assumption: Equal variances\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n### Before\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit, 3, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-44-1.png){width=960}\n:::\n:::\n\n:::\n\n:::{.column width=\"50%\"}\n\n### After\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit_log, 3, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-45-1.png){width=960}\n:::\n:::\n\n:::\n::::\n\n## Is transforming better?\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n### Before\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,\tAdjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n:::\n\n:::{.column width=\"50%\"}\n\n### After\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(Ozone) ~ Temp, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.14469 -0.33095  0.02961  0.36507  1.49421 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.83797    0.45100  -4.075 8.53e-05 ***\nTemp         0.06750    0.00575  11.741  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5848 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.5473,\tAdjusted R-squared:  0.5434 \nF-statistic: 137.8 on 1 and 114 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n:::\n::::\n\n. . .\n\n**We will expand on this in the next lecture.**\n\n# Multiple linear regression\n\n## Can we use more predictors? {auto-animate=true}\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(airquality)\n```\n\n::: {.cell-output-display}\n![](Lecture-07_files/figure-revealjs/unnamed-chunk-48-1.png){width=1920}\n:::\n:::\n\n\n\nCan we improve the current model by adding *wind* and *solar radiation* as additional predictors?\n\n## Can we use more predictors? {auto-animate=true}\n\nCan we improve the current model by adding *wind* and *solar radiation* as additional predictors?\n\n. . .\n\n### From:\n\n$$log(size)_i = \\beta_0 + \\beta_1Temp_i + \\epsilon_i$$\n\n### To:\n\n$$log(size)_i = \\beta_0 + \\beta_1Temp_i + \\color{royalblue}{\\beta_2Solar.R_i + \\beta_3Wind_i} + \\epsilon_i$$\n\n\n## Can we use more predictors? {auto-animate=true}\n\n$$log(size)_i = \\beta_0 + \\beta_1Temp_i + \\color{royalblue}{\\beta_2Solar.R_i + \\beta_3Wind_i} + \\epsilon_i$$\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmulti_fit <- lm(log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\nsummary(multi_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,\tAdjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n<br>\n\nModel estimate:\n\n$$\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind$$\n\n## Multiple linear regression {auto-animate=true}\n\nModel estimate:\n\n$$\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind$$\n\n. . .\n\n### The MLR model\n\n$$Y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + \\epsilon$$\n\nwhere\n\n- we have a response variable ($Y$) which we wish to predict using predictor variables ($x_k$)\n- $\\beta_0$ is the y-intercept\n- $\\beta_k$ is the partial regression coefficient associated with the $k^{th}$ predictor variable\n- $\\epsilon$ is error and $\\epsilon \\sim N(0,\\ \\sigma^2)$\n\n## Interpretation {auto-animate=true}\n\n$$\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind$$\n\n. . .\n\nAutomating the model using `equatiomatic`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nequatiomatic::extract_eq(multi_fit, use_coefs = TRUE, coef_digits = 3)\n```\n\n::: {.cell-output-display}\n$$\n\\operatorname{\\widehat{log(Ozone)}} = -0.262 + 0.049(\\operatorname{Temp}) + 0.003(\\operatorname{Solar.R}) - 0.062(\\operatorname{Wind})\n$$\n\n:::\n:::\n\n\n. . .\n\n**Holding all other variables constant:**\n\n- A one degree (&deg;F) increase in `Temp` is associated with a 4.9% increase in ozone concentration.\n- A one unit increase in `Solar.R` is associated with a 0.3% increase in ozone concentration.\n- A one unit increase in `Wind` is associated with a 6.2% decrease in ozone concentration.\n\n\n\n## Is MLR model better?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::tab_model(fit_log, multi_fit, digits = 4, show.ci = FALSE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"2\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">log(Ozone)</th>\n<th colspan=\"2\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">log(Ozone)</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;1.8380</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.2621</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.637</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Temp</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.0675</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.0492</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Solar R</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.0025</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Wind</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.0616</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"2\">116</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"2\">111</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> / R<sup>2</sup> adjusted</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"2\">0.547 / 0.543</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"2\">0.664 / 0.655</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n- The adjusted $R^2$ is higher for the MLR model...\n- Interpretation of $R^2$ is the same as for simple linear regression: how much of the variation in the response variable is explained by the model\n- **Are all the variables/predictors needed?**\n\n# Next lecture: Variable selection\nWe will discuss how to select the best subset of predictors for a model.\n\n\n# Thanks!\n\n**Questions? Comments?**\n\nSlides made with [Quarto](https://quarto.org)\n",
    "supporting": [
      "Lecture-07_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}