{
  "hash": "2e0591bdca36353834b6750ed9c94cb2",
  "result": {
    "markdown": "---\ntitle: \"Regression modelling\"\nsubtitle: ENVX2001 Applied Statistical Methods\nauthor: Liana Pozza\ninstitute: The University of Sydney\ndate: last-modified # today | last-modified\ndate-format: \"MMM YYYY\"\nexecute:\n  cache: false\n  echo: true\neditor-options:\n  canonical: true\ntoc: true\ntoc-depth: 1\ntoc-title: Outline\n---\n\n\n\n\n\n# Yesterday...\n\n- We discussed the basics of linear regression\n- Looked at how to define a linear relationship between two variables\n- How to fit a simple linear regression model \n- How to obtain output from the model\n- Checking assumptions\n\n  \n# Inference\n\nIs our fitted model the best representation of the relationship between the variables?\n\n## Back to Galton's data\n\nWhat can we understand about the relationship between `child` and `parent`?\n\n- 928 children of 205 pairs of parents\n- Height of parents and children measured in inches\n- Size classes were binned (hence data looks discrete)\n\n\n\n## Hypothesis testing\n\nHow does our null ($H_0: \\beta_1=0$) model compare to the linear ($H_0: \\beta_1 \\neq 0$) model?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggplot2)\nlibrary(HistData) # Historical data package, this is where we find Galton's data\n\n\nnull_model <- Galton %>%\n  lm(child ~ 1, data = .) %>%\n  augment(Galton)\nlin_model <- Galton %>%\n  lm(child ~ parent, data = .) %>%\n  augment(Galton)\nmodels <- bind_rows(null_model, lin_model) %>%\n  mutate(model = rep(c(\"Null model\", \"SLR model\"), each = nrow(Galton)))\n\nggplot(data = models, aes(x = parent, y = child)) +\n  geom_smooth(\n    data = filter(models, model == \"Null model\"),\n    method = \"lm\", se = FALSE, formula = y ~ 1, size = 0.5\n  ) +\n  geom_smooth(\n    data = filter(models, model == \"SLR model\"),\n    method = \"lm\", se = FALSE, formula = y ~ x, size = 0.5\n  ) +\n  geom_segment(\n    aes(xend = parent, yend = .fitted),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    size = 0.3, color = \"darkgray\"\n  ) +\n  geom_point(alpha = .2) +\n  facet_wrap(~model) +\n  xlab(\"Parent height (in)\") +\n  ylab(\"Child height (in)\")\n```\n\n::: {.cell-output-display}\n![](Lecture-07b_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n::: {.fragment}\nThe null model is a flat line at the mean of the child height (mean = 68 inches).\n:::\n## ANOVA using linear regression \n\nANOVA is simply a variation of the linear regression\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n### Using ANOVA\n\n`anova(fit)`\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(formula = child ~ parent, data = Galton)\nanova(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: child\n           Df Sum Sq Mean Sq F value    Pr(>F)    \nparent      1 1236.9 1236.93  246.84 < 2.2e-16 ***\nResiduals 926 4640.3    5.01                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n### Using Regression\n\n`summary(fit)`\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.94153    2.81088   8.517   <2e-16 ***\nparent       0.64629    0.04114  15.711   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,\tAdjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n:::\n::::\n\n## ANOVA using linear regression\n\nANOVA is simply a variation of the linear regression\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n### Using ANOVA\n\nThe ANOVA suggests that the main effect of parent is statistically significant and large (F(1, 926) = 246.84, p < .001)\n:::\n\n::: {.column width=\"50%\"}\n### Using Regression\nWe fitted a linear model (estimated using OLS) to predict child with parent (formula: child ~ parent). The model explains a statistically significant and moderate proportion of variance (R2 = 0.21, F(1, 926) = 246.84, p < .001, adj. R2 = 0.21). Within this model, the effect of parent is statistically significant and positive (beta = 0.65, 95% CI [0.57, 0.73], t(926) = 15.71, p < .001).\n\n:::\n::::\n\n# Patterns\nWhat if we want to predict from the data?\n\n## Model fit {auto-animate=\"true\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nCoefficients:\n(Intercept)       parent  \n    23.9415       0.6463  \n```\n:::\n:::\n\n\nTranslates to:\n\n$$\\widehat{child} = 23.9 + 0.65 \\cdot parent$$\n\n. . .\n\n- For every unit change in parent (i.e. *1 inch*), we expect a 0.65 unit change in child.\n- Note that the model is deterministic, so we can predict the value of child for *any* value of parent, *even if it doesn't make sense -- need to be careful!*\n- Error is no longer \"counted\" in the fit, although it is used to estimate the parameters.\n\n## {auto-animate=\"true\"}\n\n\n$$\\widehat{child} = 23.9 + 0.65 \\cdot parent$$\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = child ~ parent, data = Galton)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.94153    2.81088   8.517   <2e-16 ***\nparent       0.64629    0.04114  15.711   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,\tAdjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n. . .\n\n- **Multiple R^2^**: proportion of variance in the response variable that is explained by the model.\n- **Adjusted R^2^**: R^2^ adjusted for the number of predictors in the model. It only increases if the new term improves the model more than would be expected by chance - a **multiple regression** situation\n    - *always lower than R^2^*\n    - pick this one if you want to compare models\n\n<!-- It is a measure of how far the data points are from the fitted line. -->\n\n## Making predictions\n\nWhat is the predicted child height for a parent height of 70 inches?\n\n. . .\n\n\nSince:\n\n$$\\widehat{child} = 23.9 + 0.65 \\cdot parent$$\n\nThen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchild <- 23.9 + 0.65 * 70\nchild\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 69.4\n```\n:::\n:::\n\n\n. . .\n\nUse `predict()` to make predictions:\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit, data.frame(parent = 70)) # using 70 as this is the value we want to sub in and predict\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n69.18187 \n```\n:::\n:::\n\n\n. . .\n\n- Need to consider:\n    - Prediction quality\n    - Prediction performance\n    - **Week 9 - Predictive modelling**\n\n# Transformations\n\nWhat if assumptions are not met, or we want to improve the model?\n\n## What if assumptions are not met?\n\n### Violations of...\n\n- **Linearity** can cause systematically wrong predictions\n- **Homoskedasticity** makes it difficult to estimate \"true\" standard deviation of errors (i.e. noisy estimates)\n- **Normality** can compromise inferences and hypothesis testing\n\n\n## How do we solve these problems?\n\n- Use less restrictive (but more complicated) methods, e.g. generalised linear models, non-parametric techniques \n- Perform variance corrections\n- [**Transform the response variable ($Y$)** to stabilise variance and correct normality]{style=\"color: seagreen\"}\n- [**Transform the predictor variable ($x$)** if issues still exist in the diagnostics]{style=\"color: seagreen\"}\n\n:::{.callout-note}\nWe can also perform transformations to improve the model fit, but **beware of overfitting** -- we want to make reasonable predictions, not fit the data!\n:::\n\n## New example: Air quality\n\nDaily air quality measurements in New York, May to September 1973\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# library(tidyverse)\nglimpse(airquality)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 153\nColumns: 6\n$ Ozone   <int> 41, 36, 12, 18, NA, 28, 23, 19, 8, NA, 7, 16, 11, 14, 18, 14, …\n$ Solar.R <int> 190, 118, 149, 313, NA, NA, 299, 99, 19, 194, NA, 256, 290, 27…\n$ Wind    <dbl> 7.4, 8.0, 12.6, 11.5, 14.3, 14.9, 8.6, 13.8, 20.1, 8.6, 6.9, 9…\n$ Temp    <int> 67, 72, 74, 62, 56, 66, 65, 59, 61, 69, 74, 69, 66, 68, 58, 64…\n$ Month   <int> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Day     <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n```\n:::\n:::\n\n\n## Is Ozone concentration influenced by Temperature?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(airquality, aes(x = Temp, y = Ozone)) +\n  geom_point(alpha = .2, size = 3) +\n  labs(\n    x = expression(\"Temperature \" ( degree~C)), \n    y = \"Ozone (parts per billion)\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](Lecture-07b_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n## Assumption checks\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(Ozone ~ Temp, data = airquality)\nlibrary(ggfortify)\nautoplot(fit)\n```\n\n::: {.cell-output-display}\n![](Lecture-07b_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\nIs a simple linear model appropriate?\n\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance::check_model(fit)\n```\n\n::: {.cell-output-display}\n![](Lecture-07b_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\nIs a simple linear model appropriate? \n\n. . .\n\n*Depends on your threshold for what is acceptable.*\n\n## The Log transform\n\n:::{.fragment}\n- Log-linear: $Log(Y)=\\beta_0+\\beta_1x$\n  - Good: an increase of $x$ by 1 unit corresponds to a $\\beta_1$ unit increase in $log(Y)$\n  - Simple: an increase of $x$ by 1 unit corresponds to a $\\beta_1 \\times 100\\%$ increase in $Y$\n:::\n:::{.fragment}\n- Linear-log: $Y=\\beta_0+\\beta_1log(x)$\n  - An increase of $1\\%$ in $x$ corresponds to a $\\frac{\\beta_1}{100}$ increase in $Y$\n:::\n:::{.fragment}\n- Log-log: $Log(Y)=\\beta_0+\\beta_1log(x)$\n  - An increase of $1\\%$ in $x$ corresponds to a $\\beta_1\\%$ increase in $Y$\n:::\n\n## Transforming Ozone\n\nLet's log transform Ozone using the natural log.\n\n:::: {.columns}\n \n::: {.column width=\"50%\"}\n:::{.fragment}\n\n### Before\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(airquality, aes(x = Temp, y = Ozone)) +\n  geom_point(alpha = .2, size = 3) +\n  labs(\n    x = expression(\"Temperature \" ( degree~C)), \n    y = \"Ozone (parts per billion)\") +\n  geom_smooth(method = \"lm\", se = FALSE) \n```\n\n::: {.cell-output-display}\n![](Lecture-07b_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n::: {.column width=\"50%\"}\n:::{.fragment}\n\n### After\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"2\"}\nggplot(airquality, aes(x = Temp, y = log(Ozone))) +\n  geom_point(alpha = .2, size = 3) +\n  labs(\n    x = expression(\"Temperature \" ( degree~C)), \n    y = \"Ozone (parts per billion)\") +\n  geom_smooth(method = \"lm\", se = FALSE) \n```\n\n::: {.cell-output-display}\n![](Lecture-07b_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n:::\n:::\n::::\n\n## Transformations\n\nThe transformed model is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate the transformed variable\nfit_log <- lm(log(Ozone) ~ Temp, data = airquality)\nfit_log\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(Ozone) ~ Temp, data = airquality)\n\nCoefficients:\n(Intercept)         Temp  \n    -1.8380       0.0675  \n```\n:::\n:::\n\n\n...and the model equation is: \n\n$$\\widehat{log(Ozone)}=\\color{royalblue}{-1.8380 + 0.0675 \\times Temp}$$\n\n. . .\n\n> A 1 degree (&deg;F) increase in temperature is associated with a 6.75% increase in ozone concentration.\n\n## Assumption: Linearity\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n### Before\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit, 1, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07b_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n:::\n\n:::{.column width=\"50%\"}\n\n### After\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit_log, 1, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07b_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n:::\n::::\n\n\n## Assumption: Normality\n\n::::{.columns}\n:::{.column width=\"50%\"}\n### Before\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit, 2, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07b_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n:::\n\n:::{.column width=\"50%\"}\n### After\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit_log, 2, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07b_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n:::\n::::\n\n\n## Assumption: Equal variances\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n### Before\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit, 3, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07b_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n:::\n\n:::{.column width=\"50%\"}\n\n### After\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(fit_log, 3, ncol = 1) +\n  cowplot::theme_cowplot(font_size = 24)\n```\n\n::: {.cell-output-display}\n![](Lecture-07b_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n:::\n::::\n\n## Is transforming better?\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n### Before\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,\tAdjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n:::\n\n:::{.column width=\"50%\"}\n\n### After\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(Ozone) ~ Temp, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.14469 -0.33095  0.02961  0.36507  1.49421 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.83797    0.45100  -4.075 8.53e-05 ***\nTemp         0.06750    0.00575  11.741  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5848 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.5473,\tAdjusted R-squared:  0.5434 \nF-statistic: 137.8 on 1 and 114 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n:::\n::::\n\n. . .\n\n**We will expand on this in the next lecture.**\n\n# Multiple linear regression\n\n## Can we use more predictors? {auto-animate=true}\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(airquality)\n```\n\n::: {.cell-output-display}\n![](Lecture-07b_files/figure-revealjs/unnamed-chunk-23-1.png){width=1920}\n:::\n:::\n\n\n\nCan we improve the current model by adding *wind* and *solar radiation* as additional predictors?\n\n## Can we use more predictors? {auto-animate=true}\n\nCan we improve the current model by adding *wind* and *solar radiation* as additional predictors?\n\n. . .\n\n### From:\n\n$$log(size)_i = \\beta_0 + \\beta_1Temp_i + \\epsilon_i$$\n\n### To:\n\n$$log(size)_i = \\beta_0 + \\beta_1Temp_i + \\color{royalblue}{\\beta_2Solar.R_i + \\beta_3Wind_i} + \\epsilon_i$$\n\n\n## Can we use more predictors? {auto-animate=true}\n\n$$log(size)_i = \\beta_0 + \\beta_1Temp_i + \\color{royalblue}{\\beta_2Solar.R_i + \\beta_3Wind_i} + \\epsilon_i$$\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmulti_fit <- lm(log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\nsummary(multi_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,\tAdjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n<br>\n\nModel estimate:\n\n$$\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind$$\n\n## Multiple linear regression {auto-animate=true}\n\nModel estimate:\n\n$$\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind$$\n\n. . .\n\n### The MLR model\n\n$$Y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + \\epsilon$$\n\nwhere\n\n- we have a response variable ($Y$) which we wish to predict using predictor variables ($x_k$)\n- $\\beta_0$ is the y-intercept\n- $\\beta_k$ is the partial regression coefficient associated with the $k^{th}$ predictor variable\n- $\\epsilon$ is error and $\\epsilon \\sim N(0,\\ \\sigma^2)$\n\n## Interpretation {auto-animate=true}\n\n$$\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind$$\n\n. . .\n\nAutomating the model using `equatiomatic`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nequatiomatic::extract_eq(multi_fit, use_coefs = TRUE, coef_digits = 3)\n```\n\n::: {.cell-output-display}\n$$\n\\operatorname{\\widehat{log(Ozone)}} = -0.262 + 0.049(\\operatorname{Temp}) + 0.003(\\operatorname{Solar.R}) - 0.062(\\operatorname{Wind})\n$$\n:::\n:::\n\n\n. . .\n\n**Holding all other variables constant:**\n\n- A one degree (&deg;F) increase in `Temp` is associated with a 4.9% increase in ozone concentration.\n- A one unit increase in `Solar.R` is associated with a 0.3% increase in ozone concentration.\n- A one unit increase in `Wind` is associated with a 6.2% decrease in ozone concentration.\n\n\n\n## Is MLR model better?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsjPlot::tab_model(fit_log, multi_fit, digits = 4, show.ci = FALSE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"2\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">log(Ozone)</th>\n<th colspan=\"2\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">log(Ozone)</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">p</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;1.8380</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.2621</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.637</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Temp</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.0675</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.0492</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Solar R</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.0025</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">Wind</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">&#45;0.0616</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"><strong>&lt;0.001</strong></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"2\">116</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"2\">111</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> / R<sup>2</sup> adjusted</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"2\">0.547 / 0.543</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"2\">0.664 / 0.655</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n- The adjusted $R^2$ is higher for the MLR model...\n- Interpretation of $R^2$ is the same as for simple linear regression: how much of the variation in the response variable is explained by the model\n- **Are all the variables/predictors needed?**\n\n# Summing up\n\nWhat have we done today?\n\n::: {.fragment}\n- Hypothesis testing with linear models\n        + Is our model the best representation of the relationship?\n:::\n::: {.fragment}\n- Interpreting model output\n        + ANOVA vs summary to view the output\n:::\n::: {.fragment}\n- Transformations to meet assumptions and improve model fit\n:::\n::: {.fragment}\n- Multiple linear regression\n        + Do more predictors improve model fit?\n:::\n\n\n\n# Next lecture: Variable selection\nWe will discuss how to select the best subset of predictors for a model.\n\n\n# Thanks!\n\n**Questions? Comments?**\n\nSlides made with [Quarto](https://quarto.org)\n",
    "supporting": [
      "Lecture-07b_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}