---
title: "Regression modelling"
subtitle: ENVX2001 Applied Statistical Methods
author: Liana Pozza
institute: The University of Sydney
date: last-modified # today | last-modified
date-format: "MMM YYYY"
execute:
  cache: false
  echo: true
editor-options:
  canonical: true
toc: true
toc-depth: 1
toc-title: Outline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  cache = TRUE)
library(tidyverse)
ggplot2::theme_set(cowplot::theme_half_open())
# ggplot2::theme_set(ggplot2::theme_minimal())
```


# Yesterday...

- We discussed the basics of linear regression
- Looked at how to define a linear relationship between two variables
- How to fit a simple linear regression model 
- How to obtain output from the model
- Checking assumptions

  
# Inference

Is our fitted model the best representation of the relationship between the variables?

## Back to Galton's data

What can we understand about the relationship between `child` and `parent`?

- 928 children of 205 pairs of parents
- Height of parents and children measured in inches
- Size classes were binned (hence data looks discrete)



## Hypothesis testing

How does our null ($H_0: \beta_1=0$) model compare to the linear ($H_0: \beta_1 \neq 0$) model?

```{r}
#| code-fold: true

library(tidyverse)
library(broom)
library(ggplot2)
library(HistData) # Historical data package, this is where we find Galton's data


null_model <- Galton %>%
  lm(child ~ 1, data = .) %>%
  augment(Galton)
lin_model <- Galton %>%
  lm(child ~ parent, data = .) %>%
  augment(Galton)
models <- bind_rows(null_model, lin_model) %>%
  mutate(model = rep(c("Null model", "SLR model"), each = nrow(Galton)))

ggplot(data = models, aes(x = parent, y = child)) +
  geom_smooth(
    data = filter(models, model == "Null model"),
    method = "lm", se = FALSE, formula = y ~ 1, size = 0.5
  ) +
  geom_smooth(
    data = filter(models, model == "SLR model"),
    method = "lm", se = FALSE, formula = y ~ x, size = 0.5
  ) +
  geom_segment(
    aes(xend = parent, yend = .fitted),
    arrow = arrow(length = unit(0.1, "cm")),
    size = 0.3, color = "darkgray"
  ) +
  geom_point(alpha = .2) +
  facet_wrap(~model) +
  xlab("Parent height (in)") +
  ylab("Child height (in)")

```
::: {.fragment}
The null model is a flat line at the mean of the child height (mean = `r round(mean(Galton$child))` inches).
:::
## ANOVA using linear regression 

ANOVA is simply a variation of the linear regression

:::: {.columns}
::: {.column width="50%"}
### Using ANOVA

`anova(fit)`
```{r}
fit <- lm(formula = child ~ parent, data = Galton)
anova(fit)
```
:::

::: {.column width="50%"}
### Using Regression

`summary(fit)`
```{r}
summary(fit)
```
:::
::::

## ANOVA using linear regression

ANOVA is simply a variation of the linear regression

:::: {.columns}
::: {.column width="50%"}
### Using ANOVA

The ANOVA suggests that the main effect of parent is statistically significant and large (F(1, 926) = 246.84, p < .001)
:::

::: {.column width="50%"}
### Using Regression
We fitted a linear model (estimated using OLS) to predict child with parent (formula: child ~ parent). The model explains a statistically significant and moderate proportion of variance (R2 = 0.21, F(1, 926) = 246.84, p < .001, adj. R2 = 0.21). Within this model, the effect of parent is statistically significant and positive (beta = 0.65, 95% CI [0.57, 0.73], t(926) = 15.71, p < .001).

:::
::::

# Patterns
What if we want to predict from the data?

## Model fit {auto-animate="true"}

```{r}
fit
```

Translates to:

$$\widehat{child} = 23.9 + 0.65 \cdot parent$$

. . .

- For every unit change in parent (i.e. *1 inch*), we expect a 0.65 unit change in child.
- Note that the model is deterministic, so we can predict the value of child for *any* value of parent, *even if it doesn't make sense -- need to be careful!*
- Error is no longer "counted" in the fit, although it is used to estimate the parameters.

## {auto-animate="true"}


$$\widehat{child} = 23.9 + 0.65 \cdot parent$$

. . .

```{r}
summary(fit)
```

. . .

- **Multiple R^2^**: proportion of variance in the response variable that is explained by the model.
- **Adjusted R^2^**: R^2^ adjusted for the number of predictors in the model. It only increases if the new term improves the model more than would be expected by chance - a **multiple regression** situation
    - *always lower than R^2^*
    - pick this one if you want to compare models

<!-- It is a measure of how far the data points are from the fitted line. -->

## Making predictions

What is the predicted child height for a parent height of 70 inches?

. . .


Since:

$$\widehat{child} = 23.9 + 0.65 \cdot parent$$

Then:

```{r}
child <- 23.9 + 0.65 * 70
child
```

. . .

Use `predict()` to make predictions:

. . .

```{r}
predict(fit, data.frame(parent = 70)) # using 70 as this is the value we want to sub in and predict

```

. . .

- Need to consider:
    - Prediction quality
    - Prediction performance
    - **Week 9 - Predictive modelling**

# Transformations

What if assumptions are not met, or we want to improve the model?

## What if assumptions are not met?

### Violations of...

- **Linearity** can cause systematically wrong predictions
- **Homoskedasticity** makes it difficult to estimate "true" standard deviation of errors (i.e. noisy estimates)
- **Normality** can compromise inferences and hypothesis testing


## How do we solve these problems?

- Use less restrictive (but more complicated) methods, e.g. generalised linear models, non-parametric techniques 
- Perform variance corrections
- [**Transform the response variable ($Y$)** to stabilise variance and correct normality]{style="color: seagreen"}
- [**Transform the predictor variable ($x$)** if issues still exist in the diagnostics]{style="color: seagreen"}

:::{.callout-note}
We can also perform transformations to improve the model fit, but **beware of overfitting** -- we want to make reasonable predictions, not fit the data!
:::

## New example: Air quality

Daily air quality measurements in New York, May to September 1973

```{r}
# library(tidyverse)
glimpse(airquality)
```

## Is Ozone concentration influenced by Temperature?

```{r}
ggplot(airquality, aes(x = Temp, y = Ozone)) +
  geom_point(alpha = .2, size = 3) +
  labs(
    x = expression("Temperature " ( degree~C)), 
    y = "Ozone (parts per billion)") +
  geom_smooth(method = "lm", se = FALSE)
```

## Assumption checks

```{r}
fit <- lm(Ozone ~ Temp, data = airquality)
library(ggfortify)
autoplot(fit)
```

Is a simple linear model appropriate?

---

```{r}
performance::check_model(fit)
```

Is a simple linear model appropriate? 

. . .

*Depends on your threshold for what is acceptable.*

## The Log transform

:::{.fragment}
- Log-linear: $Log(Y)=\beta_0+\beta_1x$
  - Good: an increase of $x$ by 1 unit corresponds to a $\beta_1$ unit increase in $log(Y)$
  - Simple: an increase of $x$ by 1 unit corresponds to a $\beta_1 \times 100\%$ increase in $Y$
:::
:::{.fragment}
- Linear-log: $Y=\beta_0+\beta_1log(x)$
  - An increase of $1\%$ in $x$ corresponds to a $\frac{\beta_1}{100}$ increase in $Y$
:::
:::{.fragment}
- Log-log: $Log(Y)=\beta_0+\beta_1log(x)$
  - An increase of $1\%$ in $x$ corresponds to a $\beta_1\%$ increase in $Y$
:::

## Transforming Ozone

Let's log transform Ozone using the natural log.

:::: {.columns}
 
::: {.column width="50%"}
:::{.fragment}

### Before

```{r}
ggplot(airquality, aes(x = Temp, y = Ozone)) +
  geom_point(alpha = .2, size = 3) +
  labs(
    x = expression("Temperature " ( degree~C)), 
    y = "Ozone (parts per billion)") +
  geom_smooth(method = "lm", se = FALSE) 
```
:::
:::

::: {.column width="50%"}
:::{.fragment}

### After

```{r}
#| code-line-numbers: "2"
ggplot(airquality, aes(x = Temp, y = log(Ozone))) +
  geom_point(alpha = .2, size = 3) +
  labs(
    x = expression("Temperature " ( degree~C)), 
    y = "Ozone (parts per billion)") +
  geom_smooth(method = "lm", se = FALSE) 
```
:::
:::
::::

## Transformations

The transformed model is:

```{r}
# generate the transformed variable
fit_log <- lm(log(Ozone) ~ Temp, data = airquality)
fit_log
```

...and the model equation is: 

$$\widehat{log(Ozone)}=\color{royalblue}{-1.8380 + 0.0675 \times Temp}$$

. . .

> A 1 degree (&deg;F) increase in temperature is associated with a 6.75% increase in ozone concentration.

## Assumption: Linearity

::::{.columns}
:::{.column width="50%"}

### Before

```{r}
#| code-fold: true
#| fig-width: 10
#| fig-height: 9
autoplot(fit, 1, ncol = 1) +
  cowplot::theme_cowplot(font_size = 24)
```
:::

:::{.column width="50%"}

### After

```{r}
#| code-fold: true 
#| fig-width: 10
#| fig-height: 9
autoplot(fit_log, 1, ncol = 1) +
  cowplot::theme_cowplot(font_size = 24)
```
:::
::::


## Assumption: Normality

::::{.columns}
:::{.column width="50%"}
### Before
```{r}
#| code-fold: true
#| fig-width: 10
#| fig-height: 9
autoplot(fit, 2, ncol = 1) +
  cowplot::theme_cowplot(font_size = 24)
```
:::

:::{.column width="50%"}
### After
```{r}
#| code-fold: true
#| fig-width: 10
#| fig-height: 9
autoplot(fit_log, 2, ncol = 1) +
  cowplot::theme_cowplot(font_size = 24)
```
:::
::::


## Assumption: Equal variances

::::{.columns}
:::{.column width="50%"}

### Before

```{r}
#| code-fold: true
#| fig-width: 10
#| fig-height: 9
autoplot(fit, 3, ncol = 1) +
  cowplot::theme_cowplot(font_size = 24)
```
:::

:::{.column width="50%"}

### After

```{r}
#| code-fold: true
#| fig-width: 10
#| fig-height: 9
autoplot(fit_log, 3, ncol = 1) +
  cowplot::theme_cowplot(font_size = 24)
```
:::
::::

## Is transforming better?

::::{.columns}
:::{.column width="50%"}

### Before

```{r}
summary(fit)
``` 

:::

:::{.column width="50%"}

### After

```{r}
summary(fit_log)
```
:::
::::

. . .

**We will expand on this in the next lecture.**

# Multiple linear regression

## Can we use more predictors? {auto-animate=true}

. . .

```{r}
#| fig-width: 20
#| fig-height: 8
plot(airquality)
```


Can we improve the current model by adding *wind* and *solar radiation* as additional predictors?

## Can we use more predictors? {auto-animate=true}

Can we improve the current model by adding *wind* and *solar radiation* as additional predictors?

. . .

### From:

$$log(size)_i = \beta_0 + \beta_1Temp_i + \epsilon_i$$

### To:

$$log(size)_i = \beta_0 + \beta_1Temp_i + \color{royalblue}{\beta_2Solar.R_i + \beta_3Wind_i} + \epsilon_i$$


## Can we use more predictors? {auto-animate=true}

$$log(size)_i = \beta_0 + \beta_1Temp_i + \color{royalblue}{\beta_2Solar.R_i + \beta_3Wind_i} + \epsilon_i$$

. . .

```{r}
multi_fit <- lm(log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)
summary(multi_fit)
```

<br>

Model estimate:

$$\widehat{log(Ozone)}=-0.262 + 0.0492 \cdot Temp + 0.00252 \cdot Solar.R - 0.0616 \cdot Wind$$

## Multiple linear regression {auto-animate=true}

Model estimate:

$$\widehat{log(Ozone)}=-0.262 + 0.0492 \cdot Temp + 0.00252 \cdot Solar.R - 0.0616 \cdot Wind$$

. . .

### The MLR model

$$Y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_kx_k + \epsilon$$

where

- we have a response variable ($Y$) which we wish to predict using predictor variables ($x_k$)
- $\beta_0$ is the y-intercept
- $\beta_k$ is the partial regression coefficient associated with the $k^{th}$ predictor variable
- $\epsilon$ is error and $\epsilon \sim N(0,\ \sigma^2)$

## Interpretation {auto-animate=true}

$$\widehat{log(Ozone)}=-0.262 + 0.0492 \cdot Temp + 0.00252 \cdot Solar.R - 0.0616 \cdot Wind$$

. . .

Automating the model using `equatiomatic`:

```{r}
equatiomatic::extract_eq(multi_fit, use_coefs = TRUE, coef_digits = 3)
```

. . .

**Holding all other variables constant:**

- A one degree (&deg;F) increase in `Temp` is associated with a 4.9% increase in ozone concentration.
- A one unit increase in `Solar.R` is associated with a 0.3% increase in ozone concentration.
- A one unit increase in `Wind` is associated with a 6.2% decrease in ozone concentration.



## Is MLR model better?

```{r}
sjPlot::tab_model(fit_log, multi_fit, digits = 4, show.ci = FALSE)
```

- The adjusted $R^2$ is higher for the MLR model...
- Interpretation of $R^2$ is the same as for simple linear regression: how much of the variation in the response variable is explained by the model
- **Are all the variables/predictors needed?**

# Summing up

What have we done today?

::: {.fragment}
- Hypothesis testing with linear models
        + Is our model the best representation of the relationship?
:::
::: {.fragment}
- Interpreting model output
        + ANOVA vs summary to view the output
:::
::: {.fragment}
- Transformations to meet assumptions and improve model fit
:::
::: {.fragment}
- Multiple linear regression
        + Do more predictors improve model fit?
:::



# Next lecture: Variable selection
We will discuss how to select the best subset of predictors for a model.


# Thanks!

**Questions? Comments?**

Slides made with [Quarto](https://quarto.org)
