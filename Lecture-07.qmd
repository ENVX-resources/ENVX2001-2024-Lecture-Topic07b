---
title: "Regression modelling"
subtitle: ENVX2001 Applied Statistical Methods
author: Liana Pozza
institute: The University of Sydney
date: last-modified # today | last-modified
date-format: "MMM YYYY"
execute:
  cache: false
  echo: true
editor-options:
  canonical: true
toc: true
toc-depth: 1
toc-title: Outline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  cache = TRUE)
library(tidyverse)
ggplot2::theme_set(cowplot::theme_half_open())
# ggplot2::theme_set(ggplot2::theme_minimal())
```


# Brief history



![Adrien-Marie Legendre](assets/legendre.jpg)
![Carl Friedrich Gauss](assets/gauss.jpg)
![Francis Galton](assets/galton.jpg)

Adrien-Marie Legendre, Carl Friedrich Gauss & Francis Galton




## Least squares, correlation and astronomy

- Method of least squares first [**published**]{style="color: firebrick"} paper by Adrien-Marie Legendre in 1805
- Technique of least squares used by Carl Friedrich Gauss in 1809 to fit a parabola to the orbit of the asteroid Ceres
- Model fitting first [**published**]{style="color: firebrick"} by Francis Galton in 1886 to the problem of predicting the height of a child from the height of the parents

:::{.callout-note}
Many other people contributed to the development of regression analysis, but these three are the "most" well-known.
:::

## Galton's data 

```{r}
library(HistData)
dplyr::tibble(Galton)
```

- 928 children of 205 pairs of parents
- Height of parents and children measured in inches
- Size classes were binned (hence data looks discrete)

---

```{r}
library(ggplot2)
ggplot(Galton, aes(x = parent, y = child)) +
  geom_point(alpha = .2, size = 3)
```

---

```{r}
library(ggplot2)
ggplot(Galton, aes(x = parent, y = child)) +
  geom_point(alpha = .2, size = 3) + geom_smooth(method = "lm")
```

## Regression modelling in R

. . .

```{r}
fit <- lm(child ~ parent, data = Galton)
summary(fit)
```

<br> 

That's it... you have fitted a model! 

. . .

:::{.callout-caution}
### Question
But how do we assess the quality of the model?
:::


<!-- ```{r}
library(report)
report(fit)
```  -->

# Simple linear regression

## Defining a linear relationship {.nostretch}

-  Pearson correlation coefficient measures the linear correlation between two variables
- Does not distinguish different *patterns* of association, only the *strength* of the association

![](assets/correlation.png)

- Not quite usable for *predictive* modelling, or for *inference* about the relationship between variables


## Anscombe's quartet

```{r}
#| code-fold: true
library(tidyverse)
anscombe %>%
  pivot_longer(everything(), cols_vary = "slowest",
    names_to = c(".value", "set"), names_pattern = "(.)(.)") %>%
  ggplot(aes(x = x, y = y)) +
    geom_point(size = 3) +
    geom_smooth(method = "lm", se = FALSE) +
    facet_wrap(~set, ncol = 4)
```

*All of these data have a correlation coefficient of about 0.8, but **only one** of them meet the assumptions of a linear model.*

## Datasaurus Dozen

```{r}
#| code-fold: true
library(datasauRus)
ggplot(datasaurus_dozen, aes(x=x, y=y)) +
  geom_point(size = .5, alpha = .3) +
  geom_smooth(method = "lm", se = TRUE) +
  facet_wrap(~dataset, ncol = 6)
```

*All of these data have the **same** correlation coefficient, mean and s.d. but look vastly different.*

## Simple linear regression modelling {auto-animate="true"}

We want to predict an outcome $Y$ based on a predictor $x$ for $i$ number of observations: 

$$Y_i = \color{royalblue}{\beta_0 + \beta_1 x_i} +\color{red}{\epsilon_i}$$

where

$$\epsilon_i \sim N(0, \sigma^2)$$

- $Y_i$, the *response*, is an observed value of the dependent variable.
- $\beta_0$, the *constant*, is the population intercept and is **fixed**.
- $\beta_1$ is the population *slope* parameter, and like $\beta_0$, is also **fixed**.
- $\epsilon_i$ is the error associated with predictions of $y_i$, and unlike $\beta_0$ or $\beta_1$, it is *not fixed*.

. . .

:::{.callout-note}
[We tend to associate $\epsilon_i$ with the **residual**, which is a positive or negative difference from the "predicted" response, rather than error itself which is a difference from the **true** response]{style="color: red;"}
:::

## Interpreting the relationship {auto-animate="true"}

$$Y_i = \color{royalblue}{\beta_0 + \beta_1 x_i} +\color{red}{\epsilon_i}$$

[Basically, a *deterministic* straight line equation $y = c + mx$, with added *random* variation that is normally distributed]{style="color: seagreen;"}

. . .

- Response = [Prediction]{style="color: royalblue"} + [Error]{style="color: red"}
- Response = [Signal]{style="color: royalblue"} + [Noise]{style="color: red"}
- Response = [Model]{style="color: royalblue"} + [Unexplained]{style="color: red"}
- Response = [Deterministic]{style="color: royalblue"} + [Random]{style="color: red"}
- Response = [Explainable]{style="color: royalblue"} + [Everything else]{style="color: red"}




## Fitting the model {auto-animate="true"}

- The *residual* is the difference between the observed value of the response and the predicted value:

$$\hat\epsilon_i = y_i - \color{royalblue}{\hat{y}_i}$$

. . .

where $\color{royalblue}{\hat{y}_i}$ is the predicted value of $y_i$:

$$\color{royalblue}{\hat{y}_i} = \beta_0 + \beta_1 x_i$$

. . .

therefore:

$$\hat\epsilon_i = y_i - \color{royalblue}{(\beta_0 + \beta_1 x_i)}$$

. . .

- We use the **method of least squares** and minimise the sum of the squared residuals (SSR):

$$\sum_{i=1}^n \hat\epsilon_i^2 = \sum_{i=1}^n (y_i - \color{royalblue}{(\beta_0 + \beta_1 x_i)})^2$$

## {auto-animate="true"}

$$\sum_{i=1}^n \hat\epsilon_i^2 = \sum_{i=1}^n (y_i - \color{royalblue}{(\beta_0 + \beta_1 x_i)})^2$$

. . .

Finding the minimum SSR requires solving the following problem:

$$\color{firebrick}{argmin_{\beta_0, \beta_1}} \sum_{i=1}^n (y_i - \color{royalblue}{(\beta_0 + \beta_1 x_i)})^2$$

## {auto-animate="true"}

$$\color{firebrick}{argmin_{\beta_0, \beta_1}} \sum_{i=1}^n (y_i - \color{royalblue}{(\beta_0 + \beta_1 x_i)})^2$$


![[source](https://github.com/Enchufa2/ls-springs)](assets/leastsquares.gif){fig-align="center"}



## Using `lm()`

Linear regression in R is performed using the `lm()` function:

```{r}
fit <- lm(child ~ parent, data = Galton)
fit
```


. . .

<br>

Does the input of `lm()` look familiar? It should!

```{r}
#| eval: false
model <- aov(y ~ x, data)
```
## Handling `lm()` output

. . .

```{r}
names(fit)
```

. . .

We can extract the output objects using the `$` operator:

```{r}
fit$coefficients
```

```{r}
fit$call
```

## Using external packages to handle `lm()` output

. . .

The `broom` package simplifies handling of model objects by converting them into tidy data frames:
```{r}
library(broom)
summ_fit <- tidy(fit)
summ_fit
```

. . .

The `sjPlot` package is useful to create a summary table:
```{r}
library(sjPlot)
sjPlot::tab_model(fit, dv.labels = "")
```

---

The `glance` function is useful for quickly assessing model parameters:

```{r}
broom::glance(fit)
```

. . .

The `augment` function adds the fitted values and residuals to the data frame:

```{r}
augment(fit)
```

# Assessing model fit

## Assumptions

The data **must** meet certain criteria, which we often call *assumptions*. They can be remembered using **LINE**:

- **L**inearity. The relationship between $y$ and $x$ is linear.
- **I**ndependence. The errors $\epsilon$ are independent.
- **N**ormal. The errors $\epsilon$ are normally distributed.
- **E**qual Variance. At each value of $x$, the variance of $y$ is the same i.e. homoskedasticity, or constant variance.

. . .

**Notice any similarities to the assumptions of ANOVA?**

:::{.callout-tip}
All but the independence assumption can be assessed using diagnostic plots. 
:::

## Assumptions with `plot()`

```{r}
#| code-fold: true
par(mfrow= c(2, 2))
plot(fit)
```


## Assumptions using `performance`

```{r}
library(performance)
```

> With great power....

. . .

```{r}
#| eval: false
performance::check_model(fit) # check all assumptions

# check specific assumption(s)
performance::check_model(fit, check = "xxx") 
```

:::{.callout-tip}
It might be easier to specify the assumptions using the `check` argument, as the default method might use diagnostic plots that are not always easy to interpret.
:::

## Assumption: Linearity

Prior knowledge and visual inspection comes into play. Does the relationship look approximately linear?

```{r}
#| code-fold: true
ggplot(Galton, aes(x = parent, y = child)) +
  geom_point(alpha = .2, size = 3) +
  # labs(
  #   x = expression("Temperature " ( degree~C)), 
  #   y = "Ozone (parts per billion)") +
  geom_smooth(method = "lm", se = FALSE)
```

:::{.callout-tip}
After running the regression, the linearity assumption can be checked *again* by looking at a plot of the residuals against $x$ i.e. size.
:::

---

```{r}
performance::check_model(fit, check = "linearity")
```

- Where the reference line is above 0, the model *underestimates* size, and where it is below 0, it *overestimates* size.
- If the linearity assumption is **violated**, there is no reason to validate the model since it is no longer suitable for the data.


## Assumption: Independence

This assumption is addressed during experimental design, but issues like correlation between errors and patterns occurring due to time are possible

- Randomisation and proper sampling handles *most* issues with independence
- Violations may occur if observations of the same subject are related i.e. [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)
- Violations may occur in time-series data, if the same subjects are sampled i.e. [autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation)

## Assumption: Normality

![](assets/residual.jpg)

---

```{r}
performance::check_model(fit, check = c("normality", "qq"))
```

---

## Assessing normality using residuals

- **Light-tailed**: small variance in residuals, resulting in a narrow distribution
- **Heavy-tailed**: many extreme positive and negative residuals, resulting in a wide distribution
- **Left-skewed** (n shape): more data falls to the left of the mean
- **Right-skewed** (u shape): more data falls to the right of the mean

---

```{r} 
#| code-fold: true
set.seed(915)
x <- rnorm(100)
y <- 2 + 5 * x + rchisq(100, df = 2)
df <- data.frame(x, y)
performance::check_model(lm(y ~ x, data = df),
  check = c(c("normality", "qq")))
```

. . .

[Heavy-tailed]{.absolute top=200 left=320}
[Right-skewed]{.absolute top=340 left=200}

[Heavy-tailed]{.absolute bottom=200 right=0}
[Right-skewed]{.absolute bottom=300 right=200}

---

```{r} 
#| code-fold: true
set.seed(1028)
x <- rnorm(100)
y <- 2 + 5 * x + rchisq(100, df = 3) * -1
df <- data.frame(x, y)
performance::check_model(lm(y ~ x, data = df),
  check = c(c("normality", "qq")))
```

. . .

[Heavy-tailed]{.absolute bottom=170 left=120}
[Left-skewed]{.absolute bottom=350 left=250}

[Heavy-tailed]{.absolute bottom=200 right=380}
[Left-skewed]{.absolute bottom=430 right=200}

---

```{r} 
#| code-fold: true
set.seed(1028)
x <- rnorm(100)
y <- 2 + 5 * x + rnbinom(100, 10, .5)
df <- data.frame(x, y)
performance::check_model(lm(y ~ x, data = df),
  check = c(c("normality", "qq")))
```

. . .

[Light-tailed?]{.absolute bottom=200 left=50}
[Right-skewed?]{.absolute top=320 left=180}
[Outlier?]{.absolute top=120 left=400}

[Light-tailed]{.absolute bottom=200 right=320}
[Outlier?]{.absolute bottom=150 right=0}

## External resources on QQ plots

- [How to interpret a QQ plot](https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot)
- [QQ plot interpretation](https://math.illinois.edu/system/files/inline-files/Proj9AY1516-report2.pdf)

## Asumption: Equal variances

```{r}
performance::check_model(fit, check = c("homogeneity", "outliers"))
```

## What is a standardised residual?

- The standardised residual is the residual divided by the standard error of the residual.

$$Standardised\ residual = \frac{Residual}{Standard\ error\ of\ the\ residual}$$

- Number of standard deviations that the residual is from the *mean* of the residuals.
- Makes it easy to assess the **equal variances** assumption (among other things).
  
# Inference
What can we understand about the relationship between `child` and `parent`?

## Hypothesis testing

How does our null ($H_0: \beta_1=0$) model compare to the linear ($H_0: \beta_1 \neq 0$) model?

```{r}
#| code-fold: true

null_model <- Galton %>%
  lm(child ~ 1, data = .) %>%
  augment(Galton)
lin_model <- Galton %>%
  lm(child ~ parent, data = .) %>%
  augment(Galton)
models <- bind_rows(null_model, lin_model) %>%
  mutate(model = rep(c("Null model", "SLR model"), each = nrow(Galton)))

ggplot(data = models, aes(x = parent, y = child)) +
  geom_smooth(
    data = filter(models, model == "Null model"),
    method = "lm", se = FALSE, formula = y ~ 1, size = 0.5
  ) +
  geom_smooth(
    data = filter(models, model == "SLR model"),
    method = "lm", se = FALSE, formula = y ~ x, size = 0.5
  ) +
  geom_segment(
    aes(xend = parent, yend = .fitted),
    arrow = arrow(length = unit(0.1, "cm")),
    size = 0.3, color = "darkgray"
  ) +
  geom_point(alpha = .2) +
  facet_wrap(~model) +
  xlab("Parent height (in)") +
  ylab("Child height (in)")
```


## ANOVA using linear regression 

ANOVA is simply a variation of the linear regression

:::: {.columns}
::: {.column width="50%"}
### Using ANOVA

### `anova(fit)`

```{r}
fit <- lm(formula = child ~ parent, data = Galton)
anova(fit)
```
:::

::: {.column width="50%"}
### Using Regression

### `summary(fit)`
```{r}
summary(fit)
```
:::
::::

## ANOVA using linear regression

ANOVA is simply a variation of the linear regression

:::: {.columns}
::: {.column width="50%"}
### Using ANOVA

The ANOVA suggests that the main effect of parent is statistically significant and large (F(1, 926) = 246.84, p < .001)
:::

::: {.column width="50%"}
### Using Regression
We fitted a linear model (estimated using OLS) to predict child with parent (formula: child ~ parent). The model explains a statistically significant and moderate proportion of variance (R2 = 0.21, F(1, 926) = 246.84, p < .001, adj. R2 = 0.21). Within this model, the effect of parent is statistically significant and positive (beta = 0.65, 95% CI [0.57, 0.73], t(926) = 15.71, p < .001).

:::
::::

# Patterns
What if we want to predict from the data?

## Model fit {auto-animate="true"}

```{r}
fit
```

Translates to:

$$\widehat{child} = 23.9 + 0.65 \cdot parent$$

. . .

- For every unit change in parent (i.e. *1 inch*), we expect a 0.65 unit change in child.
- Note that the model is deterministic, so we can predict the value of child for *any* value of parent, *even if it doesn't make sense -- need to be careful!*
- Error is no longer "counted" in the fit, although it is used to estimate the parameters.

## {auto-animate="true"}


$$\widehat{child} = 23.9 + 0.65 \cdot parent$$

. . .

```{r}
summary(fit)
```

. . .

- **Multiple R^2^**: proportion of variance in the response variable that is explained by the model.
- **Adjusted R^2^**: R^2^ adjusted for the number of predictors in the model. It only increases if the new term improves the model more than would be expected by chance - a **multiple regression** situation
    - *always lower than R^2^*
    - pick this one if you want to compare models

<!-- It is a measure of how far the data points are from the fitted line. -->

## Making predictions

What is the predicted child height for a parent height of 70 inches?

. . .


Since:

$$\widehat{child} = 23.9 + 0.65 \cdot parent$$

Then:

```{r}
child <- 23.9 + 0.65 * 70
child
```

. . .

Use `predict()` to make predictions:

. . .

```{r}
predict(fit, data.frame(parent = 70))
```

. . .

- Need to consider:
    - Prediction quality
    - Prediction performance
    - **Week 9 - Predictive modelling**

# Transformations

What if assumptions are not met, or we want to improve the model?

## What if assumptions are not met?

### Violations of...

- **Linearity** can cause systematically wrong predictions
- **Homoskedasticity** makes it difficult to estimate "true" standard deviation of errors (i.e. noisy estimates)
- **Normality** can compromise inferences and hypothesis testing


## How do we solve these problems?

- Use less restrictive (but more complicated) methods, e.g. generalised linear models, non-parametric techniques 
- Perform variance corrections
- [**Transform the response variable ($Y$)** to stabilise variance and correct normality]{style="color: seagreen"}
- [**Transform the predictor variable ($x$)** if issues still exist in the diagnostics]{style="color: seagreen"}

:::{.callout-note}
We can also perform transformations to improve the model fit, but **beware of overfitting** -- we want to make reasonable predictions, not fit the data!
:::

## New example: Air quality

Daily air quality measurements in New York, May to September 1973

```{r}
library(tidyverse)
glimpse(airquality)
```

## Is Ozone concentration influenced by Temperature?

```{r}
ggplot(airquality, aes(x = Temp, y = Ozone)) +
  geom_point(alpha = .2, size = 3) +
  labs(
    x = expression("Temperature " ( degree~C)), 
    y = "Ozone (parts per billion)") +
  geom_smooth(method = "lm", se = FALSE)
```

## Assumption checks

```{r}
fit <- lm(Ozone ~ Temp, data = airquality)
library(ggfortify)
autoplot(fit)
```

Is a simple liner model appropriate?

---

```{r}
performance::check_model(fit)
```

Is a simple liner model appropriate? 

. . .

*Depends on your threshold for what is acceptable.*

## The Log transform

- Log-linear: $Log(Y)=\beta_0+\beta_1x$
  - Good: an increase of $x$ by 1 unit corresponds to a $\beta_1$ unit increase in $log(Y)$
  - Simple: an increase of $x$ by 1 unit corresponds to a $\beta_1 \times 100\%$ increase in $Y$
- Linear-log: $Y=\beta_0+\beta_1log(x)$
  - An increase of $1\%$ in $x$ corresponds to a $\frac{\beta_1}{100}$ increase in $Y$
- Log-log: $Log(Y)=\beta_0+\beta_1log(x)$
  - An increase of $1\%$ in $x$ corresponds to a $\beta_1\%$ increase in $Y$

## Transforming Ozone

Let's log transform Ozone using the natural log.

:::: {.columns}
 
::: {.column width="50%"}
:::{.fragment}

### Before

```{r}
ggplot(airquality, aes(x = Temp, y = Ozone)) +
  geom_point(alpha = .2, size = 3) +
  labs(
    x = expression("Temperature " ( degree~C)), 
    y = "Ozone (parts per billion)") +
  geom_smooth(method = "lm", se = FALSE) 
```
:::
:::

::: {.column width="50%"}
:::{.fragment}

### After

```{r}
#| code-line-numbers: "2"
ggplot(airquality, aes(x = Temp, y = log(Ozone))) +
  geom_point(alpha = .2, size = 3) +
  labs(
    x = expression("Temperature " ( degree~C)), 
    y = "Ozone (parts per billion)") +
  geom_smooth(method = "lm", se = FALSE) 
```
:::
:::
::::

## Transformations

The transformed model is:

```{r}
# generate the transformed variable
fit_log <- lm(log(Ozone) ~ Temp, data = airquality)
fit_log
```

...and the model equation is: 

$$\widehat{log(Ozone)}=\color{royalblue}{-1.8380 + 0.0675 \times Temp}$$

. . .

> A 1 degree (&deg;F) increase in temperature is associated with a 6.75% increase in ozone concentration.

## Assumption: Linearity

::::{.columns}
:::{.column width="50%"}

### Before

```{r}
#| code-fold: true
#| fig-width: 10
#| fig-height: 9
autoplot(fit, 1, ncol = 1) +
  cowplot::theme_cowplot(font_size = 24)
```
:::

:::{.column width="50%"}

### After

```{r}
#| code-fold: true 
#| fig-width: 10
#| fig-height: 9
autoplot(fit_log, 1, ncol = 1) +
  cowplot::theme_cowplot(font_size = 24)
```
:::
::::


## Assumption: Normality

::::{.columns}
:::{.column width="50%"}

### Before

```{r}
#| code-fold: true
#| fig-width: 10
#| fig-height: 9
autoplot(fit, 2, ncol = 1) +
  cowplot::theme_cowplot(font_size = 24)
```
:::

:::{.column width="50%"}

### After

```{r}
#| code-fold: true
#| fig-width: 10
#| fig-height: 9
autoplot(fit_log, 2, ncol = 1) +
  cowplot::theme_cowplot(font_size = 24)
```
:::
::::


## Assumption: Equal variances

::::{.columns}
:::{.column width="50%"}

### Before

```{r}
#| code-fold: true
#| fig-width: 10
#| fig-height: 9
autoplot(fit, 3, ncol = 1) +
  cowplot::theme_cowplot(font_size = 24)
```
:::

:::{.column width="50%"}

### After

```{r}
#| code-fold: true
#| fig-width: 10
#| fig-height: 9
autoplot(fit_log, 3, ncol = 1) +
  cowplot::theme_cowplot(font_size = 24)
```
:::
::::

## Is transforming better?

::::{.columns}
:::{.column width="50%"}

### Before

```{r}
summary(fit)
``` 

:::

:::{.column width="50%"}

### After

```{r}
summary(fit_log)
```
:::
::::

. . .

**We will expand on this in the next lecture.**

# Multiple linear regression

## Can we use more predictors? {auto-animate=true}

. . .

```{r}
#| fig-width: 20
#| fig-height: 8
plot(airquality)
```


Can we improve the current model by adding *wind* and *solar radiation* as additional predictors?

## Can we use more predictors? {auto-animate=true}

Can we improve the current model by adding *wind* and *solar radiation* as additional predictors?

. . .

### From:

$$log(size)_i = \beta_0 + \beta_1Temp_i + \epsilon_i$$

### To:

$$log(size)_i = \beta_0 + \beta_1Temp_i + \color{royalblue}{\beta_2Solar.R_i + \beta_3Wind_i} + \epsilon_i$$


## Can we use more predictors? {auto-animate=true}

$$log(size)_i = \beta_0 + \beta_1Temp_i + \color{royalblue}{\beta_2Solar.R_i + \beta_3Wind_i} + \epsilon_i$$

. . .

```{r}
multi_fit <- lm(log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)
summary(multi_fit)
```

<br>

Model estimate:

$$\widehat{log(Ozone)}=-0.262 + 0.0492 \cdot Temp + 0.00252 \cdot Solar.R - 0.0616 \cdot Wind$$

## Multiple linear regression {auto-animate=true}

Model estimate:

$$\widehat{log(Ozone)}=-0.262 + 0.0492 \cdot Temp + 0.00252 \cdot Solar.R - 0.0616 \cdot Wind$$

. . .

### The MLR model

$$Y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_kx_k + \epsilon$$

where

- we have a response variable ($Y$) which we wish to predict using predictor variables ($x_k$)
- $\beta_0$ is the y-intercept
- $\beta_k$ is the partial regression coefficient associated with the $k^{th}$ predictor variable
- $\epsilon$ is error and $\epsilon \sim N(0,\ \sigma^2)$

## Interpretation {auto-animate=true}

$$\widehat{log(Ozone)}=-0.262 + 0.0492 \cdot Temp + 0.00252 \cdot Solar.R - 0.0616 \cdot Wind$$

. . .

Automating the model using `equatiomatic`:

```{r}
equatiomatic::extract_eq(multi_fit, use_coefs = TRUE, coef_digits = 3)
```

. . .

**Holding all other variables constant:**

- A one degree (&deg;F) increase in `Temp` is associated with a 4.9% increase in ozone concentration.
- A one unit increase in `Solar.R` is associated with a 0.3% increase in ozone concentration.
- A one unit increase in `Wind` is associated with a 6.2% decrease in ozone concentration.



## Is MLR model better?

```{r}
sjPlot::tab_model(fit_log, multi_fit, digits = 4, show.ci = FALSE)
```

- The adjusted $R^2$ is higher for the MLR model...
- Interpretation of $R^2$ is the same as for simple linear regression: how much of the variation in the response variable is explained by the model
- **Are all the variables/predictors needed?**

# Next lecture: Variable selection
We will discuss how to select the best subset of predictors for a model.


# Thanks!

**Questions? Comments?**

Slides made with [Quarto](https://quarto.org)
